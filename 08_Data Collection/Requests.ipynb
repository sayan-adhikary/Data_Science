{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e86f6b3-7f49-4a9a-b3b8-7b34808f1188",
   "metadata": {},
   "source": [
    "# Using `requests` module for Data Collection\n",
    "Today we will see how to scrape websites and use requests module to download the raw html of a webpage. In this section we can safely use https://quotes.toscrape.com/ and https://books.toscrape.com/ for scraping demos\n",
    "\n",
    "## 1. What is `requests`?\n",
    "- requests is a Python library used to send HTTP requests easily.\n",
    "- It allows you to fetch the content of a webpage programmatically.\n",
    "- It is commonly used as the first step before parsing HTML with BeautifulSoup.\n",
    "\n",
    "## 2. Installing requests\n",
    "- To install `requests`, run:\n",
    "\n",
    "    ```pip install requests```\n",
    "\n",
    "## 3. Sending a Basic GET Request\n",
    "## Example\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the HTML content\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "### Key points:\n",
    "\n",
    "- `url`: The website you want to fetch.\n",
    "- `response.text`: The HTML content of the page as a string.\n",
    "\n",
    "## 4. Checking the Response Status\n",
    "Always check if the request was successful:\n",
    "\n",
    "```python \n",
    "print(response.status_code)\n",
    "```\n",
    "\n",
    "## Common Status Codes\n",
    "- `200`: OK (Success)\n",
    "- `404`: Not Found\n",
    "- `403`: Forbidden\n",
    "- `500`: Internal Server Error\n",
    "\n",
    "### Good practice:\n",
    "```python\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "else:\n",
    "    print(\"Failed to fetch the page.\")\n",
    "```\n",
    "## 5. Important Response Properties\n",
    "| Property            | Description                          |\n",
    "|------------------------|--------------------------------------|\n",
    "| `response.text`        | HTML content as Unicode text         |\n",
    "| `response.content`     | Raw bytes of the response            |\n",
    "| `response.status_code` | HTTP status code                  |\n",
    "| `response.headers`     | Metadata like content-type, server info |\n",
    "\n",
    "## 6. Adding Headers to Mimic a Browser\n",
    "Sometimes websites block automated requests. Adding a User-Agent header helps the request look like it is coming from a real browser.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Accessing response properties\n",
    "print(\"Text:\", response.text)\n",
    "print(\"Content (bytes):\", response.content)\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Headers:\", response.headers)\n",
    "```\n",
    "\n",
    "\n",
    "## 7. Handling Connection Errors\n",
    "Wrap your request in a try-except block to handle errors gracefully:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "    print(response.text)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "```\n",
    "\n",
    "## 8. Best Practices for Fetching Pages\n",
    "- Always check the HTTP status code.\n",
    "- Use proper headers to mimic a browser.\n",
    "- Set a timeout to avoid hanging indefinitely.\n",
    "- Respect the website by not making too many rapid requests.\n",
    "## 9. Summary\n",
    "- `requests` makes it simple to fetch web pages using Python.\n",
    "- It is the starting point for most web scraping workflows.\n",
    "- Combining `requests` with BeautifulSoup allows for powerful data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eecc073-b9e5-4bee-bbd2-4ed83dd4fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeba62d1-d30a-47c9-91ee-a1e21b84a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = requests.get(\"https://books.toscrape.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14cad715-1d80-4747-8964-257052dd92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"htmls/page1.html\", \"w\") as f:\n",
    "    f.write(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5856250-4c21-438f-b73e-2b2425d73aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded page 1 Succesfully\n",
      "Downloaded page 2 Succesfully\n",
      "Downloaded page 3 Succesfully\n",
      "Downloaded page 4 Succesfully\n",
      "Downloaded page 5 Succesfully\n",
      "Downloaded page 6 Succesfully\n",
      "Downloaded page 7 Succesfully\n",
      "Downloaded page 8 Succesfully\n",
      "Downloaded page 9 Succesfully\n",
      "Downloaded page 10 Succesfully\n",
      "Downloaded page 11 Succesfully\n",
      "Downloaded page 12 Succesfully\n",
      "Downloaded page 13 Succesfully\n",
      "Downloaded page 14 Succesfully\n",
      "Downloaded page 15 Succesfully\n",
      "Downloaded page 16 Succesfully\n",
      "Downloaded page 17 Succesfully\n",
      "Downloaded page 18 Succesfully\n",
      "Downloaded page 19 Succesfully\n",
      "Downloaded page 20 Succesfully\n",
      "Downloaded page 21 Succesfully\n",
      "Downloaded page 22 Succesfully\n",
      "Downloaded page 23 Succesfully\n",
      "Downloaded page 24 Succesfully\n",
      "Downloaded page 25 Succesfully\n",
      "Downloaded page 26 Succesfully\n",
      "Downloaded page 27 Succesfully\n",
      "Downloaded page 28 Succesfully\n",
      "Downloaded page 29 Succesfully\n",
      "Downloaded page 30 Succesfully\n",
      "Downloaded page 31 Succesfully\n",
      "Downloaded page 32 Succesfully\n",
      "Downloaded page 33 Succesfully\n",
      "Downloaded page 34 Succesfully\n",
      "Downloaded page 35 Succesfully\n",
      "Downloaded page 36 Succesfully\n",
      "Downloaded page 37 Succesfully\n",
      "Downloaded page 38 Succesfully\n",
      "Downloaded page 39 Succesfully\n",
      "Downloaded page 40 Succesfully\n",
      "Downloaded page 41 Succesfully\n",
      "Downloaded page 42 Succesfully\n",
      "Downloaded page 43 Succesfully\n",
      "Downloaded page 44 Succesfully\n",
      "Downloaded page 45 Succesfully\n",
      "Downloaded page 46 Succesfully\n",
      "Downloaded page 47 Succesfully\n",
      "Downloaded page 48 Succesfully\n",
      "Downloaded page 49 Succesfully\n",
      "Downloaded page 50 Succesfully\n"
     ]
    }
   ],
   "source": [
    "for i in range (1, 51):\n",
    "    a = requests.get(f\"https://books.toscrape.com/catalogue/page-{i}.html\")\n",
    "    with open(f\"htmls/page{i}.html\", \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(a.text)\n",
    "        print(f\"Downloaded page {i} Succesfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
